---
title: "Practical Machine Learning - Course Project"
author: "Nigel Weedon"
date: "20 November 2015"
output: html_document
---

# Introduction  
This document is the final report for the major "Course Project" assignment of Coursera's Practical Machine Learning from Johns Hopkins University.

The environment used to develop and test the embedded R code was a Toshiba Laptop(32Gb), Windows 7(64bit) and RStudio Version 0.99.467.

GitHub Repository: [Coursera Practical Machine Learning](https://github.com/weedonn/Coursera-Practical-Machine-Learning)  
GitPage: [Course Project Report](http://weedonn.github.io/Coursera-Practical-Machine-Learning/)  

# Background  
Using devices such as **_Jawbone Up_**, **_Nike FuelBand_**, and **_Fitbit_** it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behaviour, or because they are tech geeks. One thing that people regularly do is quantify how **_much_** of a particular activity they do, but they rarely quantify **_how well they do it_**. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).  

# Methodology  
The goal of the project is to predict the manner in which each of the 6 participants, mentioned in the **Background** above, did their exercise.  
This is defined by the target variable "classe" in the training dataset.  
We first explore and analyse the training data provided using the R package/tool rattle(). We also load the training data set into Microsoft Excel to home in on and analyse specific feature or predictor data.  
Based on this analysis, we cleans the data by removing whole columns of NULL data. Depending on the volume of other missing data and their location, we can impute these values and/or only retain **complete.cases**. This is where rows of data are removed if there is any missing data in the row.  
After cleansing the data we look at other techniques to reduce the number of predictors, such as identifying "near-zero-variance" predictors and "variable importance" of each predictor.  
Since the target variable is a categorical with 5 levels (A,B,C,D,E), we choose 2 model algorithms, "Random Forest(rf)" and the clustering algorithm "K-Nearest Neighbour(knn)" to train and compare models.  
We partition the training dataset into 2 sub-sets, training and validation(testing) sets, which are based on a random 70%/30% split.
To estimate the out-of-sample error rate we train a randomForest model on a small subset of the training data.
We then train each model(rf, knn) using the full training dataset and note the out-of-sample error rate.
We then use these models to predict the target variable and note the models predictive accuracy percentage.
The model with the highest accuracy and lowest out-of-sample error rate will be used to predict the final submission test data.
The accuracy and out-of-sample error rate of the models can then be tuned by iteratively including/excluding data and/or model training parameter changes.  
Because the out-of-sample error rate was initially so low, the creation of additional and/or covariate features was deemed to be unneccessary.

# Data  
There were 2 data sets provided for this project, a training data set and a testing data set.  
The training data set is available here: [pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)  

The testing data set is available here: [pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)  

The Human Activity Recognition Project at Groupware@LES have been very generous in allowing their data to be used for this assignment.  
The data for this course project comes from this source: [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har).  

# Building the Model  
## Setup the environment  
```{r Setup_the_environment}
## Clear the workspace.
rm(list=ls())
## Load the required packages.
library("caret")
library("rattle")
#rattle()
# Set the random seed.
set.seed(1618)
# Function to generate files with predictions to submit for assignment
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}
```  
# Data Exploration and Analysis  
The initial data analysis was performed via rattle(), which provides a GUI to explore the data and generates a huge number of statistics.  
We look at the data types (identity,categoric,numeric,target), unique values (levels), number and percentage of missing values, skewness of the data in each column and if there are any outliers (mean substantially different to median).
This analysis highlighted a huge number of "missing" observations and the occurance of "#DIV/0!" which we also interpret as a missing value.  
```{r Data_Exploration}
# R Code courtesy of rattle()
crs$datasetUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
crs$dataset <- read.csv(url(crs$datasetUrl), na.strings=c(".", "NA", "", "?", "#DIV/0!"), strip.white=TRUE, encoding="UTF-8")
crs$nobs <- nrow(crs$dataset) # 19622 observations 
crs$sample <- crs$train <- sample(nrow(crs$dataset), 0.7*crs$nobs) # 13735 observations
crs$validate <- sample(setdiff(seq_len(nrow(crs$dataset)), crs$train), 0.15*crs$nobs) # 2943 observations
crs$test <- setdiff(setdiff(seq_len(nrow(crs$dataset)), crs$train), crs$validate) # 2944 observations

# The following variable selections have been noted.

crs$input <- c("user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp",
     "new_window", "num_window", "roll_belt", "pitch_belt",
     "yaw_belt", "total_accel_belt", "kurtosis_roll_belt", "kurtosis_picth_belt",
     "skewness_roll_belt", "skewness_roll_belt.1", "max_roll_belt", "max_picth_belt",
     "max_yaw_belt", "min_roll_belt", "min_pitch_belt", "min_yaw_belt",
     "amplitude_roll_belt", "amplitude_pitch_belt", "amplitude_yaw_belt", "var_total_accel_belt",
     "avg_roll_belt", "stddev_roll_belt", "var_roll_belt", "avg_pitch_belt",
     "stddev_pitch_belt", "var_pitch_belt", "avg_yaw_belt", "stddev_yaw_belt",
     "var_yaw_belt", "gyros_belt_x", "gyros_belt_y", "gyros_belt_z",
     "accel_belt_x", "accel_belt_y", "accel_belt_z", "magnet_belt_x",
     "magnet_belt_y", "magnet_belt_z", "roll_arm", "pitch_arm",
     "yaw_arm", "total_accel_arm", "var_accel_arm", "avg_roll_arm",
     "stddev_roll_arm", "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm",
     "var_pitch_arm", "avg_yaw_arm", "stddev_yaw_arm", "var_yaw_arm",
     "gyros_arm_x", "gyros_arm_y", "gyros_arm_z", "accel_arm_x",
     "accel_arm_y", "accel_arm_z", "magnet_arm_x", "magnet_arm_y",
     "magnet_arm_z", "kurtosis_roll_arm", "kurtosis_picth_arm", "kurtosis_yaw_arm",
     "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm", "max_roll_arm",
     "max_picth_arm", "max_yaw_arm", "min_roll_arm", "min_pitch_arm",
     "min_yaw_arm", "amplitude_roll_arm", "amplitude_pitch_arm", "amplitude_yaw_arm",
     "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "kurtosis_roll_dumbbell",
     "kurtosis_picth_dumbbell", "skewness_roll_dumbbell", "skewness_pitch_dumbbell", "max_roll_dumbbell",
     "max_picth_dumbbell", "max_yaw_dumbbell", "min_roll_dumbbell", "min_pitch_dumbbell",
     "min_yaw_dumbbell", "amplitude_roll_dumbbell", "amplitude_pitch_dumbbell", "amplitude_yaw_dumbbell",
     "total_accel_dumbbell", "var_accel_dumbbell", "avg_roll_dumbbell", "stddev_roll_dumbbell",
     "var_roll_dumbbell", "avg_pitch_dumbbell", "stddev_pitch_dumbbell", "var_pitch_dumbbell",
     "avg_yaw_dumbbell", "stddev_yaw_dumbbell", "var_yaw_dumbbell", "gyros_dumbbell_x",
     "gyros_dumbbell_y", "gyros_dumbbell_z", "accel_dumbbell_x", "accel_dumbbell_y",
     "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y", "magnet_dumbbell_z",
     "roll_forearm", "pitch_forearm", "yaw_forearm", "kurtosis_roll_forearm",
     "kurtosis_picth_forearm", "skewness_roll_forearm", "skewness_pitch_forearm", "max_roll_forearm",
     "max_picth_forearm", "max_yaw_forearm", "min_roll_forearm", "min_pitch_forearm",
     "min_yaw_forearm", "amplitude_roll_forearm", "amplitude_pitch_forearm", "amplitude_yaw_forearm",
     "total_accel_forearm", "var_accel_forearm", "avg_roll_forearm", "stddev_roll_forearm",
     "var_roll_forearm", "avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm",
     "avg_yaw_forearm", "stddev_yaw_forearm", "var_yaw_forearm", "gyros_forearm_x",
     "gyros_forearm_y", "gyros_forearm_z", "accel_forearm_x", "accel_forearm_y",
     "accel_forearm_z", "magnet_forearm_x", "magnet_forearm_y", "magnet_forearm_z")

crs$numeric <- c("raw_timestamp_part_1", "raw_timestamp_part_2", "num_window", "roll_belt",
     "pitch_belt", "yaw_belt", "total_accel_belt", "max_roll_belt",
     "max_picth_belt", "min_roll_belt", "min_pitch_belt", "amplitude_roll_belt",
     "amplitude_pitch_belt", "var_total_accel_belt", "avg_roll_belt", "stddev_roll_belt",
     "var_roll_belt", "avg_pitch_belt", "stddev_pitch_belt", "var_pitch_belt",
     "avg_yaw_belt", "stddev_yaw_belt", "var_yaw_belt", "gyros_belt_x",
     "gyros_belt_y", "gyros_belt_z", "accel_belt_x", "accel_belt_y",
     "accel_belt_z", "magnet_belt_x", "magnet_belt_y", "magnet_belt_z",
     "roll_arm", "pitch_arm", "yaw_arm", "total_accel_arm",
     "var_accel_arm", "avg_roll_arm", "stddev_roll_arm", "var_roll_arm",
     "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
     "stddev_yaw_arm", "var_yaw_arm", "gyros_arm_x", "gyros_arm_y",
     "gyros_arm_z", "accel_arm_x", "accel_arm_y", "accel_arm_z",
     "magnet_arm_x", "magnet_arm_y", "magnet_arm_z", "max_roll_arm",
     "max_picth_arm", "max_yaw_arm", "min_roll_arm", "min_pitch_arm",
     "min_yaw_arm", "amplitude_roll_arm", "amplitude_pitch_arm", "amplitude_yaw_arm",
     "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "max_roll_dumbbell",
     "max_picth_dumbbell", "min_roll_dumbbell", "min_pitch_dumbbell", "amplitude_roll_dumbbell",
     "amplitude_pitch_dumbbell", "total_accel_dumbbell", "var_accel_dumbbell", "avg_roll_dumbbell",
     "stddev_roll_dumbbell", "var_roll_dumbbell", "avg_pitch_dumbbell", "stddev_pitch_dumbbell",
     "var_pitch_dumbbell", "avg_yaw_dumbbell", "stddev_yaw_dumbbell", "var_yaw_dumbbell",
     "gyros_dumbbell_x", "gyros_dumbbell_y", "gyros_dumbbell_z", "accel_dumbbell_x",
     "accel_dumbbell_y", "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y",
     "magnet_dumbbell_z", "roll_forearm", "pitch_forearm", "yaw_forearm",
     "max_roll_forearm", "max_picth_forearm", "min_roll_forearm", "min_pitch_forearm",
     "amplitude_roll_forearm", "amplitude_pitch_forearm", "total_accel_forearm", "var_accel_forearm",
     "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm", "avg_pitch_forearm",
     "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm", "stddev_yaw_forearm",
     "var_yaw_forearm", "gyros_forearm_x", "gyros_forearm_y", "gyros_forearm_z",
     "accel_forearm_x", "accel_forearm_y", "accel_forearm_z", "magnet_forearm_x",
     "magnet_forearm_y", "magnet_forearm_z")

crs$categoric <- c("user_name", "cvtd_timestamp", "new_window", "kurtosis_roll_belt",
     "kurtosis_picth_belt", "skewness_roll_belt", "skewness_roll_belt.1", "max_yaw_belt",
     "min_yaw_belt", "amplitude_yaw_belt", "kurtosis_roll_arm", "kurtosis_picth_arm",
     "kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
     "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "skewness_roll_dumbbell", "skewness_pitch_dumbbell",
     "max_yaw_dumbbell", "min_yaw_dumbbell", "amplitude_yaw_dumbbell", "kurtosis_roll_forearm",
     "kurtosis_picth_forearm", "skewness_roll_forearm", "skewness_pitch_forearm", "max_yaw_forearm",
     "min_yaw_forearm", "amplitude_yaw_forearm")

crs$target  <- "classe"
crs$risk    <- NULL
crs$ident   <- "X"
crs$ignore  <- c("kurtosis_yaw_belt", "skewness_yaw_belt", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "kurtosis_yaw_forearm", "skewness_yaw_forearm")
crs$weights <- NULL
```  

## Summary of the Dataset  
```{r Summary_of_the_Dataset}
# The 'Hmisc' package provides the 'contents' function.
library(Hmisc, quietly=TRUE)
# Obtain a summary of the dataset.

contents(crs$dataset[crs$sample, c(crs$input, crs$risk, crs$target)])
#summary(crs$dataset[crs$sample, c(crs$input, crs$risk, crs$target)])
```  

## Skewness of the Dataset  
NOTE: Skewness for each numeric variable of the dataset.  
Positive means the right tail is longer.  
```{r Skewness_of_the_Dataset}
# The 'skewness' package provides the 'fBasics' function.
library(fBasics, quietly=TRUE)
# Summarise the skewness of the numeric data.

skewness(crs$dataset[crs$sample, c(crs$input, crs$risk, crs$target)][,c(2:3, 6:10, 15:16, 18:19, 21:22, 24:65, 72:83, 88:89, 91:92, 94:95, 97:119, 124:125, 127:128, 130:131, 133:152)], na.rm=TRUE)

rm(crs)  

```  

## Load the Training and Testing datasets  
```{r Load_Datasets}
pml_training_Url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
pml_training <- read.csv(url(pml_training_Url), na.strings=c("NA","#DIV/0!",""))
# Remove the first 7 columns which contain Identity, Date/Time and Categorical features, which may not be predictive.
pml_training <- pml_training[, -c(1:7)]
# Discard the columns where there is < 95% observations, based on percentage missing from data analysis.
sum(colSums(!is.na(pml_training)) < nrow(pml_training) * 0.95) #columns with more than 95% data missing
cols2Keep <- c(colSums(!is.na(pml_training)) >= nrow(pml_training) * 0.95)
pml_training   <-  pml_training[,cols2Keep]
```  
## Identify "near-zero-variance" predictors.  
After removing columns with a large percentage of null data, there were no near-zero-variance columns identified for removal.  
```{r Near_Zero_Variance}
# These predictors may have an undue influence on the model and should be eliminated prior to modeling.
nzv2Remove <- nearZeroVar(pml_training, saveMetrics= TRUE)
nzv2Remove
# Identify and remove "near-zero-variance" predictors.
#pml_training <- pml_training[, nzv2Remove$nzv==FALSE]
```  
## Apply the same data cleansing to the testing data set  
```{r Cleansing_Testing_Dataset}
pml_testing_Url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
pml_testing <- read.csv(url(pml_testing_Url), na.strings=c("NA","#DIV/0!",""))
# Remove the first 7 columns which contain Identity, Date/Time and Categorical features, which may not be predictive.
pml_testing <- pml_testing[ , -c(1:7)]
# Discard the columns where there is < 95% observations
pml_testing <- pml_testing[,cols2Keep]
# Identify and remove "near-zero-variance" predictors.
#pml_testing <- pml_testing[, nzv2Remove$nzv==FALSE]
```

## Verify column names  
```{r Verify_Column_Names}
# Verify that the column names in the training and test sets are identical, excluding the classe and problem_id columns.
colnames(pml_training)
colnames(pml_testing)
```  
## Partitioning Training data set  
```{r Partition_Training_Dataset}
# Partitioning the Training data set into two data sets, 70% for training, 30% for validation(testing)
inTrain <- createDataPartition(pml_training$classe, p=0.7, list=FALSE)
training <- pml_training[inTrain, ]
testing <- pml_training[-inTrain, ]
```  
## Estimate Out-of-Sample Error  
We can use a small sample of the training data set to estimate the expected out-of-sample error.  
The randomForest algorithm performs inherent Cross Validation, but we explicitly specify Cross Validation(cv) to calculate an estimate for the out-of-sample error.
This sample gives an OOB estimate of error rate of 4.25%.  
As a rule of thumb, the more data we train with the better the predictive accuracy of the model.  
So we would expect that training a model on the full training data set will produce an Out-of-Sample error rate  substantially better than 4.25%.  
```{r Estimate_Out_of_Sample_Error}
# Partioning the training data set again into a 20% sample data set to estimate the OOB of the final model.
inSample <- createDataPartition(training$classe, p=0.2, list=FALSE)
OOBtraining <- training[inSample, ]
rfOOBModel <- train(OOBtraining$classe ~ ., method="rf", data=OOBtraining,  trControl=trainControl(method="cv"), prox=TRUE, allowParallel=TRUE, importance = TRUE)
# Output of randomForest sample model
rfOOBModel
rfOOBModel$finalModel  

```  

## Variable Importance Analysis of Random Forest Model  
The Variable Importance plots show the top 15 features.
```{r Variable_Importance_Analysis}
rfImportance <- varImp(rfOOBModel,scale=FALSE) # for train(method="rf")  
print(rfImportance$importance)  
plot(rfImportance, top = 15)  
  
```  

## Train the randomForest model - Cross Validation  
Now we train a model on the full training data set.  
We found that in previous model training runs that preprocessing the data to normalise the data(preProcess = c("center","scale")) did not affect the accuracy of the final model. Also repeated cross-validation affected the number of variables per level(mtry) used, but again this did not substantially affect the accuracy of the final model.
```{r Train_RandomForest}
randomForestModel <- train(training$classe ~ ., method="rf", data=training,  trControl=trainControl(method="cv"), prox=TRUE, allowParallel=TRUE, importance = TRUE)
# Output of randomForest trained model
randomForestModel
# Output of randomForest last model
randomForestModel$finalModel  

randomForestPredict <- predict(randomForestModel,testing)
randomForestConfMatrix <- confusionMatrix(randomForestPredict, testing$classe)
randomForestConfMatrix  

```

## Train the K-Nearest Neighbour Model - Cross Validation  
In the K-Nearest Neighbour algorithm specify Cross Validation in the trainControl parameter to calculate the out-of-sample error.
In this case Resampling: Cross-Validated (10 fold).  
We also pre-process the data ("center","scale") to normalise the data.  
```{r Train_K_Nearest_Neighbour}
# K-Nearest Neighbour Analysis(knn)
# knn requires variables to be normalized or scaled.
KNNctrl = trainControl(method="cv")
KNNmod = train(training$classe ~ ., data=training, method = "knn", trControl = KNNctrl, preProcess = c("center","scale"))
# Output of knn trained model
KNNmod  

```
Plot Number of Neighbours Vs accuracy  
Plotting yields the Number of Neighbours Vs accuracy, based on the cross validation.  
```{r Plot_Neighbours_Vs_Accuracy, echo=FALSE}
plot(KNNmod)
  
```  

Predict the Testing data set using the K-Nearest Neighbour Model  
```{r Predict_Testing_Datase_KNN_Model}
KNNpred <- predict(KNNmod,testing)
KNNConfMatrix <- confusionMatrix(KNNpred, testing$classe)
KNNConfMatrix  

```  

## Conclusion  
The Random Forest model produced the best results with an OOB error rate of 0.68% and Accuracy of 99.44%.  
While the K-Nearest Neighbour model produced an Accuracy of 96.99%.  
The Random Forest model consistently out performed the K-Nearest Neighbour Model in all tunning exercises.  

## Generate the predicted answers  
Generate the 20 prediction files to submit as answers.  
```{r Generate_Predicted_Answers}  
predictedAnswers <- predict(randomForestModel,newdata=pml_testing) # Best predictions  
#predictedAnswers <- predict(KNNmod,newdata=pml_testing)  
#predictedAnswers  
saveRDS(predictedAnswers, file="predictedAnswers.rds")
#predictedAnswers <- readRDS("predictedAnswers.rds")
pml_write_files(predictedAnswers)
```
