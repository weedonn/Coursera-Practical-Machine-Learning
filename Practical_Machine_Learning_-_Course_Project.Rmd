---
title: "Practical Machine Learning - Course Project"
author: "Nigel Weedon"
date: "17 November 2015"
output: html_document
---

# Introduction  
This document is the final report for the major "Course Project" assignment of Coursera's Practical Machine Learning from Johns Hopkins University.

The environment used to develop and test the embedded R code was a Toshiba Laptop(32Gb), Windows 7(64bit) and RStudio Version 0.99.467.

GitHub Repository: [Coursera Practical Machine Learning](https://github.com/weedonn/Coursera-Practical-Machine-Learning)  
GitPage: [Course Project Report](http://weedonn.github.io/Coursera-Practical-Machine-Learning/)  

# Background  
Using devices such as **_Jawbone Up_**, **_Nike FuelBand_**, and **_Fitbit_** it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behaviour, or because they are tech geeks. One thing that people regularly do is quantify how **_much_** of a particular activity they do, but they rarely quantify **_how well they do it_**. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).  

# Methodology  
The goal of the project is to predict the manner in which each of the 6 participants, mentioned in the **Background** above, did their exercise.  
This is defined by the target variable "classe" in the training dataset.  
We first explore and analyse the training data provided using the R package/tool rattle(). We also load the training data set into Microsoft Excel to home in on specific data.  
Based on this analysis, we cleans the data by removing whole columns of NULL data. Depending on the volume of other missing data and their location, we can impute these values and/or only retain **complete.cases**. This is where rows of data are removed if there is any missing data in the row.  
After cleansing the data we look at other techniques to reduce the number of predictors, such as identifying "near-zero-variance" predictors and "variable importance" of each predictor.  
Since the target variable is a categorical with 5 levels (A,B,C,D,E), we choose 2 model algorithms, "Random Forest(rf)" and the clustering algorithm "K-Nearest Neighbour(knn)" to train and compare models.  
We partition the training dataset into 2 sub-sets, training and validation(testing) sets, which are based on a random 70%/30% split.
To estimate the out-of-sample error rate we train a randomForest model on a small subset of the training data.
We then train each model(rf, knn) using the full training dataset and note the out-of-sample error rate.
We then use these models to predict the target variable and note the models predictive accuracy percentage.
The model with the highest accuracy and lowest out-of-sample error rate will be used to predict the final submission test data.
The accuracy and out-of-sample error rate of the models can then be tuned by iteratively including/excluding data and/or model training parameter changes.  
Because the out-of-sample error rate was initially so low, the creation of additional and/or covariate features was deemed to be unneccessary.

# Data  
There were 2 data sets provided for this project, a training data set and a testing data set.  
The training data set is available here: [pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)  

The testing data set is available here: [pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)  

The Human Activity Recognition Project at Groupware@LES have been very generous in allowing their data to be used for this assignment.  
The data for this course project comes from this source: [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har).  

# Building the Model  
## Setup the environment  
```{r}
## Set the working directory.
setwd("C:\\Users\\Nigel\\Documents\\Coursera\\Practical Machine Learning - Johns Hopkins\\Assignments\\Course_Project")
## Clear the workspace.
rm(list=ls())
## Load the required packages.
library("caret")
library("randomForest")
library("plyr")
library("rattle")
#rattle()
# Set the random seed.
set.seed(1618)
# Function to generate files with predictions to submit for assignment
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}
```  
# Data Exploration and Analysis  
The initial data analysis was performed via rattle(), which provides a GUI to explore the data and generates a huge number of statistics.  
We look at the data types (identity,categoric,numeric,target), unique values (levels), number and percentage of missing values, skewness of the data in each column and if there are any outliers (mean substantially different to median).
This analysis highlighted a huge number of "missing" observations and the occurance of "#DIV/0!" which we also interpret as a missing value.  
```{r}
# R Code courtesy of rattle()
crs$datasetUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
crs$dataset <- read.csv(url(crs$datasetUrl), na.strings=c(".", "NA", "", "?", "#DIV/0!"), strip.white=TRUE, encoding="UTF-8")
crs$nobs <- nrow(crs$dataset) # 19622 observations 
crs$sample <- crs$train <- sample(nrow(crs$dataset), 0.7*crs$nobs) # 13735 observations
crs$validate <- sample(setdiff(seq_len(nrow(crs$dataset)), crs$train), 0.15*crs$nobs) # 2943 observations
crs$test <- setdiff(setdiff(seq_len(nrow(crs$dataset)), crs$train), crs$validate) # 2944 observations

# The following variable selections have been noted.

crs$input <- c("user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp",
     "new_window", "num_window", "roll_belt", "pitch_belt",
     "yaw_belt", "total_accel_belt", "kurtosis_roll_belt", "kurtosis_picth_belt",
     "skewness_roll_belt", "skewness_roll_belt.1", "max_roll_belt", "max_picth_belt",
     "max_yaw_belt", "min_roll_belt", "min_pitch_belt", "min_yaw_belt",
     "amplitude_roll_belt", "amplitude_pitch_belt", "amplitude_yaw_belt", "var_total_accel_belt",
     "avg_roll_belt", "stddev_roll_belt", "var_roll_belt", "avg_pitch_belt",
     "stddev_pitch_belt", "var_pitch_belt", "avg_yaw_belt", "stddev_yaw_belt",
     "var_yaw_belt", "gyros_belt_x", "gyros_belt_y", "gyros_belt_z",
     "accel_belt_x", "accel_belt_y", "accel_belt_z", "magnet_belt_x",
     "magnet_belt_y", "magnet_belt_z", "roll_arm", "pitch_arm",
     "yaw_arm", "total_accel_arm", "var_accel_arm", "avg_roll_arm",
     "stddev_roll_arm", "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm",
     "var_pitch_arm", "avg_yaw_arm", "stddev_yaw_arm", "var_yaw_arm",
     "gyros_arm_x", "gyros_arm_y", "gyros_arm_z", "accel_arm_x",
     "accel_arm_y", "accel_arm_z", "magnet_arm_x", "magnet_arm_y",
     "magnet_arm_z", "kurtosis_roll_arm", "kurtosis_picth_arm", "kurtosis_yaw_arm",
     "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm", "max_roll_arm",
     "max_picth_arm", "max_yaw_arm", "min_roll_arm", "min_pitch_arm",
     "min_yaw_arm", "amplitude_roll_arm", "amplitude_pitch_arm", "amplitude_yaw_arm",
     "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "kurtosis_roll_dumbbell",
     "kurtosis_picth_dumbbell", "skewness_roll_dumbbell", "skewness_pitch_dumbbell", "max_roll_dumbbell",
     "max_picth_dumbbell", "max_yaw_dumbbell", "min_roll_dumbbell", "min_pitch_dumbbell",
     "min_yaw_dumbbell", "amplitude_roll_dumbbell", "amplitude_pitch_dumbbell", "amplitude_yaw_dumbbell",
     "total_accel_dumbbell", "var_accel_dumbbell", "avg_roll_dumbbell", "stddev_roll_dumbbell",
     "var_roll_dumbbell", "avg_pitch_dumbbell", "stddev_pitch_dumbbell", "var_pitch_dumbbell",
     "avg_yaw_dumbbell", "stddev_yaw_dumbbell", "var_yaw_dumbbell", "gyros_dumbbell_x",
     "gyros_dumbbell_y", "gyros_dumbbell_z", "accel_dumbbell_x", "accel_dumbbell_y",
     "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y", "magnet_dumbbell_z",
     "roll_forearm", "pitch_forearm", "yaw_forearm", "kurtosis_roll_forearm",
     "kurtosis_picth_forearm", "skewness_roll_forearm", "skewness_pitch_forearm", "max_roll_forearm",
     "max_picth_forearm", "max_yaw_forearm", "min_roll_forearm", "min_pitch_forearm",
     "min_yaw_forearm", "amplitude_roll_forearm", "amplitude_pitch_forearm", "amplitude_yaw_forearm",
     "total_accel_forearm", "var_accel_forearm", "avg_roll_forearm", "stddev_roll_forearm",
     "var_roll_forearm", "avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm",
     "avg_yaw_forearm", "stddev_yaw_forearm", "var_yaw_forearm", "gyros_forearm_x",
     "gyros_forearm_y", "gyros_forearm_z", "accel_forearm_x", "accel_forearm_y",
     "accel_forearm_z", "magnet_forearm_x", "magnet_forearm_y", "magnet_forearm_z")

crs$numeric <- c("raw_timestamp_part_1", "raw_timestamp_part_2", "num_window", "roll_belt",
     "pitch_belt", "yaw_belt", "total_accel_belt", "max_roll_belt",
     "max_picth_belt", "min_roll_belt", "min_pitch_belt", "amplitude_roll_belt",
     "amplitude_pitch_belt", "var_total_accel_belt", "avg_roll_belt", "stddev_roll_belt",
     "var_roll_belt", "avg_pitch_belt", "stddev_pitch_belt", "var_pitch_belt",
     "avg_yaw_belt", "stddev_yaw_belt", "var_yaw_belt", "gyros_belt_x",
     "gyros_belt_y", "gyros_belt_z", "accel_belt_x", "accel_belt_y",
     "accel_belt_z", "magnet_belt_x", "magnet_belt_y", "magnet_belt_z",
     "roll_arm", "pitch_arm", "yaw_arm", "total_accel_arm",
     "var_accel_arm", "avg_roll_arm", "stddev_roll_arm", "var_roll_arm",
     "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
     "stddev_yaw_arm", "var_yaw_arm", "gyros_arm_x", "gyros_arm_y",
     "gyros_arm_z", "accel_arm_x", "accel_arm_y", "accel_arm_z",
     "magnet_arm_x", "magnet_arm_y", "magnet_arm_z", "max_roll_arm",
     "max_picth_arm", "max_yaw_arm", "min_roll_arm", "min_pitch_arm",
     "min_yaw_arm", "amplitude_roll_arm", "amplitude_pitch_arm", "amplitude_yaw_arm",
     "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "max_roll_dumbbell",
     "max_picth_dumbbell", "min_roll_dumbbell", "min_pitch_dumbbell", "amplitude_roll_dumbbell",
     "amplitude_pitch_dumbbell", "total_accel_dumbbell", "var_accel_dumbbell", "avg_roll_dumbbell",
     "stddev_roll_dumbbell", "var_roll_dumbbell", "avg_pitch_dumbbell", "stddev_pitch_dumbbell",
     "var_pitch_dumbbell", "avg_yaw_dumbbell", "stddev_yaw_dumbbell", "var_yaw_dumbbell",
     "gyros_dumbbell_x", "gyros_dumbbell_y", "gyros_dumbbell_z", "accel_dumbbell_x",
     "accel_dumbbell_y", "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y",
     "magnet_dumbbell_z", "roll_forearm", "pitch_forearm", "yaw_forearm",
     "max_roll_forearm", "max_picth_forearm", "min_roll_forearm", "min_pitch_forearm",
     "amplitude_roll_forearm", "amplitude_pitch_forearm", "total_accel_forearm", "var_accel_forearm",
     "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm", "avg_pitch_forearm",
     "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm", "stddev_yaw_forearm",
     "var_yaw_forearm", "gyros_forearm_x", "gyros_forearm_y", "gyros_forearm_z",
     "accel_forearm_x", "accel_forearm_y", "accel_forearm_z", "magnet_forearm_x",
     "magnet_forearm_y", "magnet_forearm_z")

crs$categoric <- c("user_name", "cvtd_timestamp", "new_window", "kurtosis_roll_belt",
     "kurtosis_picth_belt", "skewness_roll_belt", "skewness_roll_belt.1", "max_yaw_belt",
     "min_yaw_belt", "amplitude_yaw_belt", "kurtosis_roll_arm", "kurtosis_picth_arm",
     "kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
     "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "skewness_roll_dumbbell", "skewness_pitch_dumbbell",
     "max_yaw_dumbbell", "min_yaw_dumbbell", "amplitude_yaw_dumbbell", "kurtosis_roll_forearm",
     "kurtosis_picth_forearm", "skewness_roll_forearm", "skewness_pitch_forearm", "max_yaw_forearm",
     "min_yaw_forearm", "amplitude_yaw_forearm")

crs$target  <- "classe"
crs$risk    <- NULL
crs$ident   <- "X"
crs$ignore  <- c("kurtosis_yaw_belt", "skewness_yaw_belt", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell", "kurtosis_yaw_forearm", "skewness_yaw_forearm")
crs$weights <- NULL
```  

## Summary of the Dataset  
```{r}
# The 'Hmisc' package provides the 'contents' function.
library(Hmisc, quietly=TRUE)
# Obtain a summary of the dataset.

contents(crs$dataset[crs$sample, c(crs$input, crs$risk, crs$target)])
#summary(crs$dataset[crs$sample, c(crs$input, crs$risk, crs$target)])
```  

## Skewness of the Dataset  
NOTE: Skewness for each numeric variable of the dataset.  
Positive means the right tail is longer.  
```{r}
# The 'skewness' package provides the 'fBasics' function.
library(fBasics, quietly=TRUE)
# Summarise the skewness of the numeric data.

skewness(crs$dataset[crs$sample, c(crs$input, crs$risk, crs$target)][,c(2:3, 6:10, 15:16, 18:19, 21:22, 24:65, 72:83, 88:89, 91:92, 94:95, 97:119, 124:125, 127:128, 130:131, 133:152)], na.rm=TRUE)
```  

## Load the Training and Testing datasets  
```{r}
pml_training_Url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
pml_training <- read.csv(url(pml_training_Url), na.strings=c("NA","#DIV/0!",""))
# Remove the first 2 columns which contain Identity and Categorical features, which are not predictive.
pml_training <- pml_training[, -c(1:2)]
# Discard the columns where there is < 95% observations, based on percentage missing from data analysis.
sum(colSums(!is.na(pml_training)) < nrow(pml_training) * 0.95) #columns with more than 95% data missing
cols2Keep <- c(colSums(!is.na(pml_training)) >= nrow(pml_training) * 0.95)
pml_training   <-  pml_training[,cols2Keep]
```  
## Identify "near-zero-variance" predictors.  
After removing columns with null data, only one other column was identified for removal (new_window).  
Later model tuning found this column added additional accuracy, so was not removed in the final model training.  
```{r}
# These predictors may have an undue influence on the model and should be eliminated prior to modeling.
nzv2Remove <- nearZeroVar(pml_training, saveMetrics= TRUE)
nzv2Remove
# Identify and remove "near-zero-variance" predictors.
#pml_training <- pml_training[, nzv2Remove$nzv==FALSE]
```  
## Apply the same data cleansing to the testing data set  
```{r}
pml_testing_Url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
pml_testing <- read.csv(url(pml_testing_Url), na.strings=c("NA","#DIV/0!",""))
# Remove the first 2 columns which contain Identity and Categorical features, which are not predictive.
pml_testing <- pml_testing[ , -c(1:2)]
# Discard the columns where there is < 95% observations
pml_testing <- pml_testing[,cols2Keep]
# Identify and remove "near-zero-variance" predictors.
#pml_testing <- pml_testing[, nzv2Remove$nzv==FALSE]
```

## Verify column names  
```{r}
# Verify that the column names in the training and test sets are identical, excluding the classe and problem_id columns.
colnames(pml_training)
colnames(pml_testing)
```  
## Partioning Training data set  
```{r}
# Partioning the Training data set into two data sets, 70% for training, 30% for validation(testing)
inTrain <- createDataPartition(pml_training$classe, p=0.7, list=FALSE)
training <- pml_training[inTrain, ]
testing <- pml_training[-inTrain, ]
```  
## Estimate Out-of-Sample Error  
We can use a small sample of the training data set to estimate the expected out-of-sample error.  
This sample gives an OOB estimate of error rate of 1.31%.  
As a rule of thumb, the more data we train with the better the predictive accuracy of the model.  
So we would expect that training a model on the full training data set will produce an Out-of-Sample error rate  substantially better than 1.31%.  
```{r}
# Partioning the training data set again into a 20% sample data set to estimate the OOB of the final model.
inSample <- createDataPartition(training$classe, p=0.2, list=FALSE)
OOBtraining <- training[inSample, ]
rfOOBModel <- randomForest(OOBtraining$classe ~.,data=OOBtraining)
# Output of randomForest sample model
rfOOBModel
```  
## Train the randomForest model - Cross Validation  
Now we train a model on the full training data set.  
The randomForest algorithm performs inherent Cross Validation to calculate the out-of-sample error.
```{r}
randomForestModel <- randomForest(training$classe ~.,data=training, importance = TRUE)
# Output of randomForest trained model
randomForestModel
randomForestPredict <- predict(randomForestModel,testing)
randomForestConfMatrix <- confusionMatrix(randomForestPredict, testing$classe)
randomForestConfMatrix
```

## Variable Importance Analysis of Random Forest Model  
The Variable Importance plots below confirms the decision to include the predictor columns 3-7 in the data for modelling. These date/time predictors were initially removed, but model tuning found these predictors increased the accuracy and reduced the out-of-sample error rate.
```{r echo=FALSE}
varImpPlot(randomForestModel)  
  
```  

## Train the K-Nearest Neighbour Model - Cross Validation  
The K-Nearest Neighbour algorithm performs inherent Cross Validation to calculate the out-of-sample error.
In this case Resampling: Cross-Validated (10 fold, repeated 5 times)  
```{r}
# K-Nearest Neighbour Analysis(knn)
# knn requires variables to be normalized or scaled.
KNNctrl = trainControl(method="repeatedcv",repeats = 5)
KNNmod = train(training$classe ~ ., data=training, method = "knn", trControl = KNNctrl, preProcess = c("center","scale"))
# Output of knn trained model
KNNmod
```
Plot Number of Neighbours Vs accuracy  
Plotting yields the Number of Neighbours Vs accuracy, based on repeated cross validation.  
```{r echo=FALSE}
plot(KNNmod)
  
```  

Predict the Testing data set using the K-Nearest Neighbour Model  
```{r}
KNNpred <- predict(KNNmod,testing)
KNNConfMatrix <- confusionMatrix(KNNpred, testing$classe)
KNNConfMatrix
```  

## Conclusion  
The Random Forest model produced the best results with an OOB error rate of 0.15% and Accuracy of 99.93%.  
While the K-Nearest Neighbour model produced an Accuracy of 97.55%.  
Initilal data analysis deemed that the first 7 columns of the data set would not add any predictive value to the models as they were identity, categorical or date/time variables. During model tuning itrations these predictors were added back into the model training dataset which proved to give a significant improvement in accuracy. The final model only excluded the first 2 columns which were the identity(X) and the categorical(user_name) columns.  

## Generate the predicted answers  
```{r}
# The predict() on the final answers pml_testing set failed because the randomForest() function
# has a bug that can only be fixed by merging the levels of the training and pml_testing sets factors.
# See http://stats.stackexchange.com/questions/29446/random-forest-and-new-factor-levels-in-test-set
# Exclude "classe" target factor (last column)..

for(attr in colnames(training[,-ncol(training)])) 
{
        if (is.factor(training[[attr]]))
        {
                new.levels <- setdiff(levels(training[[attr]]), levels(pml_testing[[attr]]))
                if ( length(new.levels) == 0 )
                        { print(paste('Factor: ',attr, ' has no new levels.')) }
                else
                {
                        print(c(paste('Factor: ',attr, ' has ', length(new.levels), ' new level(s), e.g.'), head(new.levels, 2)))
                        levels(pml_testing[[attr]]) <- union(levels(pml_testing[[attr]]), levels(training[[attr]]))
                }
        }
}  

predictedAnswers <- predict(randomForestModel,newdata=pml_testing) # Best predictions
#predictedAnswers <- predict(KNNmod,newdata=pml_testing)
```  
Generate the 20 prediction files to submit  
```{r}
pml_write_files(predictedAnswers)
```
